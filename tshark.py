import re
import json
import subprocess
import os
from glob import glob
from multiprocessing import Pool, cpu_count
from concurrent.futures import ThreadPoolExecutor
from datetime import datetime
import shutil


def extract_conv(layer, pcap_file):
    """tsharkë¥¼ ì´ìš©í•´ íŠ¹ì • ë ˆì´ì–´ì˜ ëŒ€í™”(conversation) ì •ë³´ë¥¼ ì¶”ì¶œ"""
    program = "C:\\Program Files\\Wireshark\\tshark.exe"

    command = [
        program,
        "-r", pcap_file, 
        "-q", 
        "-z", f"conv,{layer}",
        "-o", "nameres.mac_name:FALSE"
    ]

    result = subprocess.run(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)

    if result.returncode != 0:
        raise Exception(f"Layer {layer} Error: {result.stderr}")
    
    return result.stdout


def split_pcap(input_file, output_dir, chunk_size=1000000):
    """editcapì„ ì´ìš©í•´ pcap íŒŒì¼ì„ chunk_size ê°œì˜ íŒ¨í‚· ë‹¨ìœ„ë¡œ ë¶„í• """
    program = "C:\\Program Files\\Wireshark\\editcap.exe"
    os.makedirs(output_dir, exist_ok=True)

    base_name = os.path.basename(input_file)
    base_name_no_ext = os.path.splitext(base_name)[0]
    output_pattern = os.path.join(output_dir, base_name_no_ext)
    split_file_pcap = output_pattern + os.path.splitext(base_name)[1]

    command = [program, "-c", str(chunk_size), input_file, split_file_pcap]
    result = subprocess.run(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)

    if result.returncode != 0:
        print(f"editcap Error: {result.stderr}")
        return []

    split_files = glob(os.path.join(output_dir, f"{base_name_no_ext}_*"))
    return split_files


def change_byte(bytes):
    """'10 MB', '5 kB' ê°™ì€ ë¬¸ìì—´ì„ ë°”ì´íŠ¸ ë‹¨ìœ„ ì •ìˆ˜ë¡œ ë³€í™˜"""
    data = bytes.split()
    unit_map = {"bytes": 1, "kB": 1024, "MB": 1024**2, "GB": 1024**3}
    return int(data[0].replace(",", "")) * unit_map[data[1]]


def parse_conv(layer, tshark_output):
    """tshark ì¶œë ¥ ê²°ê³¼ë¥¼ JSON ë°ì´í„°ë¡œ ë³€í™˜"""
    pattern = re.compile(
        r'([0-9a-fA-F.:]+(?:\:\d+)?) +<-> +([0-9a-fA-F.:]+(?:\:\d+)?) +(\d+) +([\d,]+ (?:GB|MB|kB|bytes)) +(\d+) +([\d,]+ (?:GB|MB|kB|bytes)) +([\d,]+) +([\d,]+ (?:GB|MB|kB|bytes)) +(\d+.\d+) +(\d+.\d+)'
    )

    data = []
    for match in pattern.findall(tshark_output):
        src_ip, dst_ip = match[0], match[1]

        if layer in ["tcp", "udp"]:
            src_ip, src_port = src_ip.rsplit(":", 1)
            dst_ip, dst_port = dst_ip.rsplit(":", 1)
            conversation = {
                "address A": src_ip,
                "port A": src_port,
                "address B": dst_ip,
                "port B": dst_port
            }
        else:
            conversation = {
                "address A": src_ip,
                "address B": dst_ip
            }

        conversation.update({
            "bytes": change_byte(match[7]),
            "bytes_atob": change_byte(match[5]),
            "bytes_btoa": change_byte(match[3]),
            "packets": int(match[6]),
            "packets_atob": int(match[4]),
            "packets_btoa": int(match[2]),
            "rel_start": float(match[8]),
            "duration": float(match[9]),
            "stream_id": -1
        })

        data.append(conversation)

    return {layer: data}


def process_layer(layer, pcap_chunk):
    """í•˜ë‚˜ì˜ ë ˆì´ì–´ë¥¼ ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜ (ë©€í‹°ìŠ¤ë ˆë”©ìš©)"""
    try:
        tshark_output = extract_conv(layer, pcap_chunk)
        convs = parse_conv(layer, tshark_output)
        return layer, convs
    except Exception as e:
        print(f"Error processing {pcap_chunk} for {layer}: {e}")
        return layer, {}


def process_pcap_chunk(pcap_chunk):
    """í•˜ë‚˜ì˜ pcap ì¡°ê°ì„ ë¶„ì„í•˜ëŠ” í•¨ìˆ˜ (ë©€í‹°ìŠ¤ë ˆë”©)"""
    layers = ["eth", "ip", "ipv6", "tcp", "udp"]
    result = {}

    # ê° ë ˆì´ì–´ì— ëŒ€í•´ ë©€í‹°ìŠ¤ë ˆë”©ì„ ì‚¬ìš©
    with ThreadPoolExecutor(max_workers=2) as executor:
        futures = [executor.submit(process_layer, layer, pcap_chunk) for layer in layers]

    # ê° ìŠ¤ë ˆë“œì˜ ê²°ê³¼ë¥¼ í•©ì¹¨
    for future in futures:
        layer, convs = future.result()
        if convs:
            result[layer] = convs[layer]

    return result


def analyze_pcap_file(pcap_file, output_folder):
    """í•˜ë‚˜ì˜ PCAP íŒŒì¼ì„ ë¶„í•  í›„ ë³‘ë ¬ ë¶„ì„ ë° ê²°ê³¼ í•©ì¹˜ê¸°"""
    print(f"Splitting {pcap_file}...")

    split_dir = os.path.join(output_folder, "split")
    split_pcaps = split_pcap(pcap_file, split_dir)

    if not split_pcaps:
        print(f"ë¶„í• ëœ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {pcap_file}")
        return

    # ğŸ”¹ `ThreadPoolExecutor`ë¥¼ ì‚¬ìš©í•˜ì—¬ ë©€í‹°ìŠ¤ë ˆë”© ì²˜ë¦¬
    results = []
    with Pool(processes=cpu_count()) as pool:
        results = pool.map(process_pcap_chunk, split_pcaps)

    merged_results = merge_results(results)

    output_file = os.path.join(output_folder, f"{os.path.basename(pcap_file)}.json")
    with open(output_file, 'w') as json_file:
        json.dump(merged_results, json_file, indent=4)

    shutil.rmtree(split_dir, ignore_errors=True)

def combine_packets(layer, convs, tsp_min):
    combined = {}  # ì†¡ìˆ˜ì‹  ìŒì„ ì €ì¥í•˜ëŠ” ë”•ì…”ë„ˆë¦¬
    after_combined = []  # ìµœì¢… ê²°ê³¼ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
    
    for conv in convs:
        if layer in ["tcp", "udp"]:
            key = frozenset([
                (conv["Address A"], conv["Port A"]), 
                (conv["Address B"], conv["Port B"])
            ])  # TCP/UDPì˜ ê²½ìš°, í¬íŠ¸ê¹Œì§€ í¬í•¨í•œ í‚¤ ìƒì„±
        else:
            key = frozenset([conv["Address A"], conv["Address B"]])  # ì¼ë°˜ì ì¸ MAC ì£¼ì†Œ ë¹„êµ

        if key in combined:
            combined[key]["bytes"] += conv["bytes"]
            combined[key]["bytes_atob"] += conv["bytes_atob"]
            combined[key]["bytes_btoa"] += conv["bytes_btoa"]
            combined[key]["packets"] += conv["packets"]
            combined[key]["packets_atob"] += conv["packets_atob"]
            combined[key]["packets_btoa"] += conv["packets_btoa"]
            combined[key]["duration"] = max(combined[key]["duration"], conv["duration"])

            # after_combinedì—ì„œ Address A, Bê°€ ë™ì¼í•œ í•­ëª©ì„ ì°¾ì•„ ì—…ë°ì´íŠ¸
            for pk in after_combined:
                if layer in ["tcp", "udp"]:
                    match = frozenset([
                        (pk["Address A"], pk["Port A"]), 
                        (pk["Address B"], pk["Port B"])
                    ]) == key
                else:
                    match = frozenset([pk["Address A"], pk["Address B"]]) == key

                if match:
                    pk["bytes"] = combined[key]["bytes"]
                    pk["bytes_atob"] = combined[key]["bytes_atob"]
                    pk["bytes_btoa"] = combined[key]["bytes_btoa"]
                    pk["packets"] = combined[key]["packets"]
                    pk["packets_atob"] = combined[key]["packets_atob"]
                    pk["packets_btoa"] = combined[key]["packets_btoa"]
                    # ê°€ì¥ í° duration ê°’ ìœ ì§€
                    pk["duration"] = max(pk["duration"], combined[key]["duration"])
                    break  # ì—…ë°ì´íŠ¸ê°€ ì™„ë£Œë˜ë©´ ë£¨í”„ ì¢…ë£Œ (ì„±ëŠ¥ í–¥ìƒ)
        else:
            new_entry = {
                "Address A": conv["Address A"],
                "Address B": conv["Address B"],
                "bytes": conv["bytes"],
                "bytes_atob": conv["bytes_atob"],
                "bytes_btoa": conv["bytes_btoa"],
                "packets": conv["packets"],
                "packets_atob": conv["packets_atob"],
                "packets_btoa": conv["packets_btoa"],
                "rel_start": conv["rel_start"],  # ìµœì´ˆê°’ ê·¸ëŒ€ë¡œ ì €ì¥
                "duration": conv["duration"],
                "stream_id": -1
            }
            if layer in ["tcp", "udp"]:
                new_entry["Port A"] = conv["Port A"]
                new_entry["Port B"] = conv["Port B"]

            combined[key] = new_entry  # ë”•ì…”ë„ˆë¦¬ì—ë„ ì €ì¥
            after_combined.append(new_entry)  # ë¦¬ìŠ¤íŠ¸ì—ë„ ì €ì¥
            
    # ë”•ì…”ë„ˆë¦¬ë¥¼ rel_start ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ í›„ ìˆœì„œëŒ€ë¡œ ì¸ë±ìŠ¤ ë¶€ì—¬
    after_combined.sort(key=lambda x: float(x["rel_start"]))
    for index, i in enumerate(after_combined):        
        i["duration"] = i["duration"] - i["rel_start"] # ìƒëŒ€ ì‹œê°„ìœ¼ë¡œ ë³€ê²½
        i["rel_start"] = i["rel_start"] - tsp_min # ìƒëŒ€ ì‹œê°„ìœ¼ë¡œ ë³€ê²½       
        i["stream_id"] = index

    return after_combined  # ë¦¬ìŠ¤íŠ¸ í˜•íƒœ ìœ ì§€

def merge_results(all_results):
    merged_data = {layer: {} for layer in ["eth", "ip", "ipv6", "tcp", "udp"]}

    # ë¦¬ìŠ¤íŠ¸ ì•ˆì— ì—¬ëŸ¬ ë”•ì…”ë„ˆë¦¬ê°€ ìˆëŠ” ê²½ìš° í•´ê²°
    for result in all_results:
        for layer, conversations in result.items():
            if layer not in merged_data:
                merged_data[layer] = {}

            for conv in conversations:
                # 'tcp' ë˜ëŠ” 'udp'ì¼ ê²½ìš°, port ì •ë³´ë¥¼ í¬í•¨í•œ key ìƒì„±
                if layer in ["tcp", "udp"]:
                    key = tuple(sorted([conv["address A"], conv["port A"], conv["address B"], conv["port B"]]))
                else:
                    # ë‹¤ë¥¸ ë ˆì´ì–´ì¼ ê²½ìš°, í¬íŠ¸ ì •ë³´ ì—†ì´ address A, address Bë§Œ ë¹„êµ
                    key = tuple(sorted([conv["address A"], conv["address B"]]))

                # ëŒ€í™”ê°€ ì²˜ìŒì´ë©´ ë³µì‚¬í•´ì„œ ì¶”ê°€, ê¸°ì¡´ì— ìˆìœ¼ë©´ ë°ì´í„° ë³‘í•©
                if key not in merged_data[layer]:
                    merged_data[layer][key] = {
                        **conv.copy(),  # ì „ì²´ ë°ì´í„°ë¥¼ ë³µì‚¬
                        "rel_start": conv["rel_start"]  # rel_startëŠ” ë”°ë¡œ ì²˜ë¦¬
                    }

                else:
                    existing = merged_data[layer][key]

                    # address A, address Bê°€ ë°”ë€Œì—ˆì„ ê²½ìš° ì²˜ë¦¬
                    if (conv["address A"], conv.get("port A", "")) == (existing["address B"], existing.get("port B", "")) and \
                       (conv["address B"], conv.get("port B", "")) == (existing["address A"], existing.get("port A", "")):
                        # ë°”ë€ ê²½ìš°ì—ëŠ” bytes_atob, packets_atobì™€ bytes_btoa, packets_btoaë¥¼ êµí™˜í•´ì„œ í•©ì¹¨
                        existing["bytes_atob"] += conv["bytes_btoa"]
                        existing["bytes_btoa"] += conv["bytes_atob"]
                        existing["packets_atob"] += conv["packets_btoa"]
                        existing["packets_btoa"] += conv["packets_atob"]
                    else:
                        # ë°”ë€Œì§€ ì•Šì€ ê²½ìš°ëŠ” ê¸°ì¡´ ë°©ì‹ëŒ€ë¡œ í•©ì¹¨
                        existing["bytes_atob"] += conv["bytes_atob"]
                        existing["bytes_btoa"] += conv["bytes_btoa"]
                        existing["packets_atob"] += conv["packets_atob"]
                        existing["packets_btoa"] += conv["packets_btoa"]

                    # ë‚˜ë¨¸ì§€ ë°ì´í„°ë„ í•©ì¹¨
                    existing["bytes"] += conv["bytes"]
                    existing["packets"] += conv["packets"]
                    existing["duration"] = existing["duration"]

    # stream_id ì¬ì •ë ¬
    for layer in merged_data:
        sorted_convs = sorted(merged_data[layer].values(), key=lambda x: x["rel_start"])
        for i, conv in enumerate(sorted_convs):
            conv["stream_id"] = i
        merged_data[layer] = sorted_convs  # ë”•ì…”ë„ˆë¦¬ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜

    return merged_data


def analyze_pcap_files(input_folder, output_folder):
    """PCAP ë° PCAPNG íŒŒì¼ ë‹¨ìœ„ë¡œ ë©€í‹°í”„ë¡œì„¸ì‹±ì„ ìˆ˜í–‰í•˜ëŠ” í•¨ìˆ˜"""
    pcap_files = [os.path.join(input_folder, f) for f in os.listdir(input_folder) if f.endswith((".pcap", ".pcapng"))]

    if not pcap_files:
        print("No PCAP or PCAPNG files found.")
        return

    # ìˆœì°¨ì ìœ¼ë¡œ ê° pcap íŒŒì¼ì„ ì²˜ë¦¬
    for pcap_file in pcap_files:
        analyze_pcap_file(pcap_file, output_folder)


if __name__ == "__main__":
    input_folder = "D:\\script\\wireshark\\pcaps"
    output_folder = "D:\\script\\wireshark\\pcap_results"
    os.makedirs(output_folder, exist_ok=True)

    start = datetime.now()
    analyze_pcap_files(input_folder, output_folder)
    end = datetime.now()

    print(f"ì‹œì‘ì‹œê°„ : {start.strftime('%H:%M:%S')}")
    print(f"ì¢…ë£Œì‹œê°„ : {end.strftime('%H:%M:%S')}")
